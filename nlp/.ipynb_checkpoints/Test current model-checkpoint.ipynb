{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "flexible-singing",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "DIR_PATH = './'\n",
    "DATA_TRAIN_PATH = os.path.join(DIR_PATH, 'data/10_cate/Train_Full/')\n",
    "DATA_TEST_PATH = os.path.join(DIR_PATH, 'data/10_cate/Test_Full/')\n",
    "DATA_TRAIN_JSON = os.path.join(DIR_PATH, 'data/json/data_train.json')\n",
    "DATA_TEST_JSON = os.path.join(DIR_PATH, 'data/json/data_test.json')\n",
    "STOP_WORDS = os.path.join(DIR_PATH, 'stopwords-nlp-vi.txt')\n",
    "SPECIAL_CHARACTER = '0123456789%@$.,=+-!;/()*\"&^:#|\\n\\t\\''\n",
    "DICTIONARY_PATH = './model/another_topic/dictionary.txt'\n",
    "MODEL_PATH ='model/linear_svc_model.pkl'\n",
    "ESTIMATE_PATH ='model/another_topic/estimator_01.h5' \n",
    "LABEL_BIN_PATH ='model/another_topic/mlb_01.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "mental-music",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from gensim import corpora\n",
    "import pickle as cPickle\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report\n",
    "from pyvi import ViTokenizer\n",
    "from gensim import matutils\n",
    "\n",
    "class FileStore(object):\n",
    "    def __init__(self, file_path, data = None):\n",
    "        self.file_path = file_path\n",
    "        self.data = data\n",
    "\n",
    "    def store_json(self):\n",
    "        with open(self.file_path, 'w') as outfile:\n",
    "            json.dump(self.data, outfile)\n",
    "\n",
    "    def store_dictionary(self, dict_words):\n",
    "        dictionary = corpora.Dictionary(dict_words)\n",
    "        dictionary.filter_extremes(no_below=20, no_above=0.3)\n",
    "        dictionary.save_as_text(self.file_path)\n",
    "\n",
    "    def save_pickle(self,  obj):\n",
    "        outfile = open(self.file_path, 'wb')\n",
    "        fastPickler = cPickle.Pickler(outfile, 4)\n",
    "        fastPickler.fast = 1\n",
    "        fastPickler.dump(obj)\n",
    "        outfile.close()\n",
    "\n",
    "class Classifier(object):\n",
    "    def __init__(self, features_train = None, labels_train = None, features_test = None, labels_test = None,  estimator = LinearSVC(random_state=0)):\n",
    "        self.features_train = features_train\n",
    "        self.features_test = features_test\n",
    "        self.labels_train = labels_train\n",
    "        self.labels_test = labels_test\n",
    "        self.estimator = estimator\n",
    "    \n",
    "    def load_model(self, model_path):  \n",
    "        estimator_file = open(model_path, \"rb\") \n",
    "        self.estimator = cPickle.Unpickler(estimator_file).load() \n",
    "        estimator_file.close()\n",
    "        # self.test(\"abc\")\n",
    "        self.__training_result()\n",
    "\n",
    "    def training(self):\n",
    "        self.estimator.fit(self.features_train, self.labels_train)\n",
    "        self.__training_result()\n",
    "\n",
    "    def save_model(self):\n",
    "        print(type(object),\"model saved\")\n",
    "        FileStore(file_path=MODEL_PATH).save_pickle(obj=object)\n",
    "        FileStore(file_path=ESTIMATE_PATH).save_pickle(obj=self.estimator)\n",
    "\n",
    "    def __training_result(self):\n",
    "        y_true, y_pred = self.labels_test, self.estimator.predict(self.features_test)\n",
    "        print(classification_report(y_true, y_pred))\n",
    "\n",
    "class FileReader(object):\n",
    "    def __init__(self, file_path, encoder = None):\n",
    "        self.file_path = file_path\n",
    "        self.encoder = encoder if encoder != None else 'utf-16le'\n",
    "\n",
    "    def read(self):\n",
    "        with open(self.file_path,'rb') as f:\n",
    "            s = f.read() \n",
    "        return s\n",
    "\n",
    "    def content(self):\n",
    "        s = self.read() \n",
    "        return s.decode(self.encoder)\n",
    "\n",
    "    def read_json(self):\n",
    "        with open(self.file_path) as f:\n",
    "            s = json.load(f)\n",
    "        return s\n",
    "\n",
    "    def read_stopwords(self):\n",
    "        with open(self.file_path, 'r') as f:\n",
    "            stopwords = set([w.strip().replace(' ', '_') for w in f.readlines()])\n",
    "        return stopwords\n",
    "\n",
    "    def load_dictionary(self):\n",
    "        return corpora.Dictionary.load_from_text(self.file_path)\n",
    "\n",
    "    def load_model(self):\n",
    "        with open(self.file_path, mode='rb') as f:\n",
    "            model = Classifier(cPickle.load(f))\n",
    "            f.close() \n",
    "        return model\n",
    "\n",
    "    def load_estimator(self): \n",
    "        #print(self.file_path)\n",
    "        with open(self.file_path, mode='rb') as f:\n",
    "            estimator = cPickle.load(f)\n",
    "            f.close()\n",
    "        return estimator\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "together-roots",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLP(object):\n",
    "    def __init__(self, text = None):\n",
    "        self.text = text\n",
    "        self.__set_stopwords()\n",
    "\n",
    "    def __set_stopwords(self):\n",
    "        self.stopwords = FileReader(STOP_WORDS).read_stopwords()\n",
    "\n",
    "    def segmentation(self):\n",
    "        return ViTokenizer.tokenize(self.text)\n",
    "\n",
    "    def split_words(self):\n",
    "        text = self.segmentation()\n",
    "        try:\n",
    "            return [x.strip(SPECIAL_CHARACTER).lower() for x in text.split()]\n",
    "        except TypeError:\n",
    "            return []\n",
    "\n",
    "    def get_words_feature(self):\n",
    "        split_words = self.split_words()\n",
    "        return [word for word in split_words if word.encode('utf-8') not in self.stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "suited-karma",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureExtraction(object):\n",
    "    def __init__(self, data=None):\n",
    "        self.data = data\n",
    "\n",
    "    def __build_dictionary(self):\n",
    "        print('Building dictionary')\n",
    "        dict_words = []\n",
    "        i = 0\n",
    "        print( self.data)\n",
    "        for text in self.data:\n",
    "            i += 1\n",
    "            print(\"Step {} / {}\".format(i, len(self.data)))\n",
    "            words = NLP(text = text['content']).get_words_feature()\n",
    "            dict_words.append(words)\n",
    "        FileStore(file_path=DICTIONARY_PATH).store_dictionary(dict_words)\n",
    "\n",
    "    def __load_dictionary(self):\n",
    "        #print(\" os.path.exists(DICTIONARY_PATH) == False: \" +  str(os.path.exists(DICTIONARY_PATH) == False))\n",
    "        if os.path.exists(DICTIONARY_PATH) == False:\n",
    "            self.__build_dictionary()\n",
    "        self.dictionary = FileReader(DICTIONARY_PATH).load_dictionary()\n",
    "\n",
    "    def __build_dataset(self):\n",
    "        self.features = []\n",
    "        self.labels = []\n",
    "        i = 0\n",
    "        for d in self.data:\n",
    "            i += 1\n",
    "            if i%500 == 0:\n",
    "                print(\"Step {} / {}\".format(i, len(self.data)))\n",
    "            self.features.append(self.get_dense(d['content']))\n",
    "            self.labels.append(d['category'])\n",
    "\n",
    "    def get_dense(self, text):\n",
    "        self.__load_dictionary()\n",
    "        words = NLP(text).get_words_feature()\n",
    "        # Bag of words\n",
    "        vec = self.dictionary.doc2bow(words) \n",
    "        dense = list(matutils.corpus2dense([vec], num_terms=len(self.dictionary)).T[0])\n",
    "        return dense\n",
    "\n",
    "    def get_data_and_label(self):\n",
    "        self.__build_dataset()\n",
    "        return self.features, self.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "authorized-madrid",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " os.path.exists(DICTIONARY_PATH) == False: False\n",
      "./model/another_topic/estimator_01.h5\n"
     ]
    },
    {
     "ename": "UnpicklingError",
     "evalue": "invalid load key, 'H'.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-869c9ccf261a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFeatureExtraction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_dense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mestimator\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'./model/another_topic/estimator_01.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_estimator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#pred = estimator.predict([a])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-38-ae35281a3cd2>\u001b[0m in \u001b[0;36mload_estimator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m             \u001b[0mestimator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcPickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m             \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnpicklingError\u001b[0m: invalid load key, 'H'."
     ]
    }
   ],
   "source": [
    "\n",
    "text = u\" Bộ Y tế chiều 6/4 ghi nhận 11 ca dương tính nCoV, đều được cách ly ngay sau nhập cảnh tại Long An, Cà Mau, Đà Nẵng và Tây Ninh.\"\n",
    "\n",
    "a = FeatureExtraction().get_dense(text)\n",
    "\n",
    "estimator =  FileReader(file_path='./model/another_topic/estimator_01.h5').load_estimator()  \n",
    "    \n",
    "#pred = estimator.predict([a]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "tracked-details",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'posixpath' has no attribute 'exist'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-bafbb886e26a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./model/another_topic/dictionary.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: module 'posixpath' has no attribute 'exist'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.path.exist('./model/another_topic/dictionary.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "behind-protocol",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
