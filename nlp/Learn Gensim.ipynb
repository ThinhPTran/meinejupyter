{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "victorian-demographic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gensim is a Python library for topic modelling, document \n",
    "# indexing and similarity retrival with large corpora. \n",
    "# Target audience is the natural language processing (NLP)\n",
    "# and information retrival (IR) community."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afraid-gambling",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features\n",
    "# * All algorithms are memory-independent. The corpus size \n",
    "# (can process input larger than RAM, streamed, out-of-core)\n",
    "# * Intuitive interfaces\n",
    "#    * easy to plug in your own input corpus/datastream (simple stream API)\n",
    "#    * easy to extend with other Vector Space algorithms (simple transform API)\n",
    "# * Efficient multicore implementations of popular algorithms\n",
    "#   such as online atent Semantic Analysis (LSA/LSI/SVD),\n",
    "#   Latent Dirichlet Allocation (LDA), Random Projections (RP),\n",
    "#   Hierarchical Dirichlet Process (HDP) or word2vec deep \n",
    "#   learning.\n",
    "# * Distributed computing: can run Latent Semantic Analysis and \n",
    "#   Latent Dirichlet Allocation on a cluster of computers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "improving-village",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Vector space model\n",
    "# Vector space model or term vector model is an algebraic model\n",
    "# for representing text documents (and any objects, in general)\n",
    "# as vectors of identifers (such as index terms). It is used \n",
    "# in information filtering, information retrieval, indexing\n",
    "# and relevancy rankings. Its first use was in the SMART\n",
    "# Information Retrieval System.\n",
    "\n",
    "# Definitions\n",
    "# Documents and queries are represented as vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "thick-lodge",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$d_j = (w_{1,j}, w_{2,j}, ... w_{t,j})$\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%latex\n",
    "$d_j = (w_{1,j}, w_{2,j}, ... w_{t,j})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "registered-paintball",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each dimension corresponds to a separate term. If a term \n",
    "# occurs in the document, its value in the the vector is \n",
    "# non-zero. Several different ways of computing these values,\n",
    "# also known as (term) weights, have been developed. One of\n",
    "# the best known schemes is tf-idf weighting.\n",
    "\n",
    "# The definition of term depends on the application. Typically \n",
    "# terms are single words, keywords, or longer phrases. If words\n",
    "# are chosen to be the terms, the dimensionality of the vector\n",
    "# is the number of words in the vocabulary (the number of \n",
    "# distinct words occuring in the corpus).\n",
    "\n",
    "# Vector operations can be used to compare documents with queries.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "quarterly-relaxation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Application\n",
    "# Relevance rankings of documents in a keyword search can be \n",
    "# calculated, using the assumptions of document similarities\n",
    "# theory, by comparing the deviation of angles between \n",
    "# each document vector and the original query vector\n",
    "# where the the query is represented as a vector with \n",
    "# same dimension as the vectors that represent the other \n",
    "# documents. \n",
    "# In practice, it is easier to calculate the cosine of the \n",
    "# angle between the vectors, instead of the angle itself.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "regulation-artist",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$cos\\theta = \\frac{d_2.q}{||d_2||.||q||}$\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%latex\n",
    "$cos\\theta = \\frac{d_2.q}{||d_2||.||q||}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "respiratory-disco",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advantages\n",
    "# The vector space model has the following advantages over \n",
    "# the Standard Boolean model:\n",
    "# 1.  Simple model based on linear algebra\n",
    "# 2.  Term weights not binary\n",
    "# 3.  Allows computing a continuous degree of similarity \n",
    "#     between queries and documents.\n",
    "# 4.  Allows ranking documents according to their possible \n",
    "#     relevance.\n",
    "# 5.  Allows partial matching.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "headed-neighborhood",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Topic modelling for humans\n",
    "# * Train large-scale semantic NLP models\n",
    "# * Represent text as semantic vectors\n",
    "# * Find semantically related documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "united-undergraduate",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core concepts\n",
    "# The core concepts of gensim are: \n",
    "# 1. Document: some text.\n",
    "# 2. Corpus: a collection of documents.\n",
    "# 3. Vector: a mathematically convenient representation of \n",
    "# a document.\n",
    "# 4. Model: an algorithm for transforming vectors from \n",
    "# one representation to another. \n",
    "# Let's examine each of these in slightly more detail. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "selected-contents",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document \n",
    "# In Gensim, a document is an object of the text sequence type\n",
    "# (commonly known as str in Python 3). A document could be \n",
    "# anything from a short 140 character tweet, a single paragraph\n",
    "# (i.e., journal artical abstract), a news artical, or a book. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "blocked-midnight",
   "metadata": {},
   "outputs": [],
   "source": [
    "document = \"Human machine interface for lab abc computer applications\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "mathematical-seeking",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corpus\n",
    "# A corpus is a collection of Document objects. Corpora serve\n",
    "# two roles in Gensim:\n",
    "# 1. Input for training a Model. During training, the models \n",
    "# this training corpus to look for common themes and topics,\n",
    "# initializing their internal model parameters. \n",
    "# \n",
    "#    Gensim focuses on unsupervised models so that no human \n",
    "#    intervention, such as costly annotations or tagging \n",
    "#    documents by hand, is required. \n",
    "# 2. Documents to organize. After training, a topic model can \n",
    "# be used to extract topics from new documents (documents not\n",
    "# seen in the training corpus).\n",
    "#    Such corpora can be indexed for Similarity Queries, \n",
    "#    queried by semantic similarity, clustered etc. \n",
    "# Here is an example corpus. It consists of 9 documents, where\n",
    "# ech document is a string consisting of a single sentence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "successful-savage",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_corpus = [\n",
    "    \"Human machine interface for lab abc computer applications\",\n",
    "    \"A survey of user opinion of computer system response time\",\n",
    "    \"The EPS user interface management system\",\n",
    "    \"System and human system engineering testing of EPS\",\n",
    "    \"Relation of user perceived response time to error measurement\",\n",
    "    \"The generation of random binary unordered trees\",\n",
    "    \"The intersection graph of paths in trees\",\n",
    "    \"Graph minors IV Widths of trees and well quasi ordering\",\n",
    "    \"Graph minors A survey\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "outer-isolation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Important\n",
    "# The above example loads the entire corpus into memory. In\n",
    "# practice, corpora may be very large, so loading them into \n",
    "# memory may be impossible. Gensim intelligently handles \n",
    "# such corpora by streaming them one document at a time.\n",
    "# See Corpus Streaming - One Document at a Time for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "continuous-infrared",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a particularly small example of a corpus for \n",
    "# illustration purposes. Another example could be a list \n",
    "# of all the plays written by Shakespear, list of all \n",
    "# wikipedia articles, or all tweets by a particular person \n",
    "# of interest. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "seven-vaccine",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After collecting our corpus, there are typically a number of \n",
    "# preprocessing steps we want to undertake. We'll keep it \n",
    "# simple and just remove some commonly used English words (\n",
    "# such as 'the') and words that occur only once in the corpus. \n",
    "# In the process of doing so, we'll tokenize our data. \n",
    "# Tokenization breaks up the documents into words (in this \n",
    "# case using space as a delimeter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "solid-monitor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Important\n",
    "# There are better ways to perform preprocessing than just \n",
    "# lower-casing and splitting by space. Effective preprocessing\n",
    "# is beyond the scope of this tutorial: if you're interested, \n",
    "# check out the gensim.utils.simple_preprocess() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "concrete-peeing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a set of frequent words\n",
    "stoplist = set('for a of the and to in'.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "gentle-porcelain",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a', 'and', 'for', 'in', 'of', 'the', 'to'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stoplist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "consistent-summer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lowercase each document, split it by whitespace and filter\n",
    "# out stopwords\n",
    "texts = [[word for word in document.lower().split() if word not in stoplist]\n",
    "        for document in text_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "transsexual-blast",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['human', 'machine', 'interface', 'lab', 'abc', 'computer', 'applications'],\n",
       " ['survey', 'user', 'opinion', 'computer', 'system', 'response', 'time'],\n",
       " ['eps', 'user', 'interface', 'management', 'system'],\n",
       " ['system', 'human', 'system', 'engineering', 'testing', 'eps'],\n",
       " ['relation', 'user', 'perceived', 'response', 'time', 'error', 'measurement'],\n",
       " ['generation', 'random', 'binary', 'unordered', 'trees'],\n",
       " ['intersection', 'graph', 'paths', 'trees'],\n",
       " ['graph', 'minors', 'iv', 'widths', 'trees', 'well', 'quasi', 'ordering'],\n",
       " ['graph', 'minors', 'survey']]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "moderate-plasma",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count word frequencies\n",
    "from collections import defaultdict\n",
    "frequency = defaultdict(int)\n",
    "for text in texts:\n",
    "    for token in text:\n",
    "        frequency[token] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fitted-celebrity",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only keep words that appear more than once\n",
    "processed_corpus = [[token for token in text if frequency[token]>1]\n",
    "                   for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "crude-interest",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['human', 'interface', 'computer'],\n",
       " ['survey', 'user', 'computer', 'system', 'response', 'time'],\n",
       " ['eps', 'user', 'interface', 'system'],\n",
       " ['system', 'human', 'system', 'eps'],\n",
       " ['user', 'response', 'time'],\n",
       " ['trees'],\n",
       " ['graph', 'trees'],\n",
       " ['graph', 'minors', 'trees'],\n",
       " ['graph', 'minors', 'survey']]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "statewide-wildlife",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before proceeding, we want to associate each word in the \n",
    "# corpus with a unique integer ID. We can do this using \n",
    "# the gensim.corpora.Dictionary class. This dictionary defines\n",
    "# the vocabulary of all words that our processing know about."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "found-thailand",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(12 unique tokens: ['computer', 'human', 'interface', 'response', 'survey']...)\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora\n",
    "dictionary = corpora.Dictionary(processed_corpus)\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "hydraulic-robert",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because our corpus is small, there are only 12 different\n",
    "# tokens in this gensim.corpora.Dictionary. For larger corpuses,\n",
    "# dictionaries that contains hundreds of thousands of tokens\n",
    "# are quite common."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "similar-prompt",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector\n",
    "# To infer the latent structure in our corpus we need a way \n",
    "# to represent documents that we can manipulate mathematically. \n",
    "# One approach is to represent each document as a vector of\n",
    "# features. For example, a single feature may be thought of \n",
    "# as a question-answer pair: \n",
    "# 1. How many times does the word splonge appear in the \n",
    "# document? Zero\n",
    "# 2. How many paragraphs does the document consist of? Two\n",
    "# 3. How many fonts does the document use? Five\n",
    "# The question is usually represented only by its integer id \n",
    "# (such as 1, 2 and 3). The representation of this document \n",
    "# becomes a series of pairs like (1, 0.0), (2, 2.0), (3, 5.0).\n",
    "# This is known as a dense vector, because it contains an \n",
    "# explicit answer to each of the above questions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "behavioral-punishment",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we know all the questions in advance, we may leave them \n",
    "# implicit and simply represent the document as (0, 2, 5). \n",
    "# This sequence of answers is the vector for our document\n",
    "# (in this case a 3-dimenional dense vector). For practical\n",
    "# purposes, only questions to which the answer is (or \n",
    "# can be converted to) a single floating point number \n",
    "# are allowed in Gensim.\n",
    "# In practice, vectors often consist of many zero values. \n",
    "# To save memory, Gensim omits all vector elements with value\n",
    "# 0.0. The above example thus becomes (2, 2.0), (3, 5.0). This\n",
    "# is known as a spare vector or bag-of-words vector. The values\n",
    "# of all missing features in this sparse representation\n",
    "# can be unambiguously resolved to zero, 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "super-magnet",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming the questions are the same, we can compare the \n",
    "# vectors of two different documents to each other. For \n",
    "# example, assume we are given two vectors (0.0, 2.0, 5.0)\n",
    "# and (0.1, 1.9, 4.9). Because the vectors are very similar\n",
    "# to each other, we can conclude that the documents \n",
    "# corresponding to those vectors are similar, too. Of course, \n",
    "# the correctness of that conclusion depends on how well we \n",
    "# picked the questions in the first place. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "devoted-rover",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another approach to represent a document as a vector is \n",
    "# the bag-of-words model. Under the bag-of-words model \n",
    "# each document is represented by a vector containing the \n",
    "# frequency counts of each word in the dictionary. For \n",
    "# example, assume we have a dictionary containing the words\n",
    "# ['coffee', 'milk', 'sugar', 'spoon']. A document consisting\n",
    "# of the string \"coffee milk coffee\" would then be represented\n",
    "# by the vector [2, 1, 0, 0] where the entries of vector are \n",
    "# (in order) the occurrences of \"coffee\", \"milk\", \"sugar\"\n",
    "# and \"spoon\" in the document. The lenght of the vector is the\n",
    "# number of entries in the dictionary. One of the main properties\n",
    "# of the bag-of-words model is that it completely ignores the\n",
    "# order of the tokens in the document that is encoded, which is\n",
    "# where the name bag-of-words comes from. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "proprietary-colonial",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our processed corpus has 12 unique words in it, which means\n",
    "# that each document will be represented by 12-dimensional\n",
    "# vector under the bag-of-words model. We can use the dictionary\n",
    "# to turn tokenized documents into these 12-dimensional vectors.\n",
    "# We can see what these IDs correspond to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "looking-prediction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'computer': 0,\n",
      " 'eps': 8,\n",
      " 'graph': 10,\n",
      " 'human': 1,\n",
      " 'interface': 2,\n",
      " 'minors': 11,\n",
      " 'response': 3,\n",
      " 'survey': 4,\n",
      " 'system': 5,\n",
      " 'time': 6,\n",
      " 'trees': 9,\n",
      " 'user': 7}\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "pprint.pprint(dictionary.token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "sacred-training",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For example, suppose we wanted to vectorize the phrase \n",
    "# \"Human computer interaction\" (note that this phrase was \n",
    "# not in our original corpus). We can create the bag-of-word\n",
    "# representation for a document using the doc2bow method of \n",
    "# the dictionary, which returns a sparse representation of the\n",
    "# word counts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "broad-control",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 1), (1, 1)]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_doc = \"Human computer interaction\"\n",
    "new_vec = dictionary.doc2bow(new_doc.lower().split())\n",
    "new_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "three-mainland",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first entry in each tuple corresponds to the ID of the \n",
    "# token in the dictionary, the second corresponds to the count\n",
    "# of this token. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "lasting-board",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that \"interaction\" did not occur in the original corpus\n",
    "# and so it was not included in the vectorization. Also note\n",
    "# that this vector only contains entries for words that acctually\n",
    "# appeared in the document. Because any given document will only\n",
    "# contain a few words out of the many words in the dictionary, \n",
    "# words that do not appear in the vectorization are represented\n",
    "# as implicitly zero as a space saving measure. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "featured-access",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can convert our entire original corpus to a list of vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "impressive-compression",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1), (1, 1), (2, 1)],\n",
      " [(0, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1)],\n",
      " [(2, 1), (5, 1), (7, 1), (8, 1)],\n",
      " [(1, 1), (5, 2), (8, 1)],\n",
      " [(3, 1), (6, 1), (7, 1)],\n",
      " [(9, 1)],\n",
      " [(9, 1), (10, 1)],\n",
      " [(9, 1), (10, 1), (11, 1)],\n",
      " [(4, 1), (10, 1), (11, 1)]]\n"
     ]
    }
   ],
   "source": [
    "bow_corpus = [dictionary.doc2bow(text) for text in processed_corpus]\n",
    "pprint.pprint(bow_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "taken-mention",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that while this list lives entirely in memory, in most \n",
    "# applications you will want a more scalable solution. Luckily, \n",
    "# gensim allows you to use any iterator that returns a single\n",
    "# document vector at a time. See the documentation for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "dense-satisfaction",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Important!\n",
    "# Depending on how the representation was obtained, two \n",
    "# different documents may have the same vector representations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fabulous-essence",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "# Now that we have vectorized our corpus we can begin to\n",
    "# transform it using models. We use model as an abstract \n",
    "# term referring to a transformation from one document \n",
    "# representation to another. In gensim documents are \n",
    "# represented as vectors so a model can be thought of as \n",
    "# a transformation between two vector spaces. The model learns\n",
    "# the details of this transformation during training, when\n",
    "# it reads the training Corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "accurate-class",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One simple example of a model is tf-idf. The tf-idf model\n",
    "# transforms vectors from the bag-of-words representation\n",
    "# to a vector space where the frequency counts are weighted\n",
    "# according to the relative rarity of each word in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "quantitative-seeking",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here's a simple example. Let's initialize the tf-idf model,\n",
    "# training it on our corpus and transforming the string\n",
    "# \"system minors\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "pleased-webcam",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(5, 0.5898341626740045), (11, 0.8075244024440723)]\n"
     ]
    }
   ],
   "source": [
    "from gensim import models\n",
    "\n",
    "# train the model\n",
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "\n",
    "# transform the \"system minors\" string\n",
    "words = \"system minors\".lower().split()\n",
    "print(tfidf[dictionary.doc2bow(words)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "behind-robert",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The tfidf model again returns a list of tuples, where the \n",
    "# first entry is the token ID and the second entry is the \n",
    "# tf-idf weighting. Note that the ID corresponding to \"system\"\n",
    "# (which occured 4 times in the original corpus) has been \n",
    "# weighted lower than the ID corresponding to minors (which \n",
    "# only occured twice).\n",
    "\n",
    "# you can save trained models to disk and later load them back\n",
    "# either to continue training on new training documents or \n",
    "# to transform new documents\n",
    "\n",
    "# gensim offers a number of different models/transformations. \n",
    "# for more, see Topics and Transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "pretty-incentive",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once you've created the model, you can do all sorts of cool \n",
    "# stuff with it. For example, to transform the whole corpus\n",
    "# via Tfldf and index, in preparation for similarity queries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "accompanied-taiwan",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import similarities\n",
    "index = similarities.SparseMatrixSimilarity(tfidf[bow_corpus], num_features=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "soviet-present",
   "metadata": {},
   "outputs": [],
   "source": [
    "# and to query the similarity of our query document\n",
    "# query_document against every document in the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "fatal-mother",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_document = 'system engineering'.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "exact-spirit",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_bow = dictionary.doc2bow(query_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "approved-anthony",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.0), (1, 0.32448703), (2, 0.41707572), (3, 0.7184812), (4, 0.0), (5, 0.0), (6, 0.0), (7, 0.0), (8, 0.0)]\n"
     ]
    }
   ],
   "source": [
    "sims = index[tfidf[query_bow]]\n",
    "print(list(enumerate(sims)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "steady-retrieval",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Human machine interface for lab abc computer applications',\n",
       " 'A survey of user opinion of computer system response time',\n",
       " 'The EPS user interface management system',\n",
       " 'System and human system engineering testing of EPS',\n",
       " 'Relation of user perceived response time to error measurement',\n",
       " 'The generation of random binary unordered trees',\n",
       " 'The intersection graph of paths in trees',\n",
       " 'Graph minors IV Widths of trees and well quasi ordering',\n",
       " 'Graph minors A survey']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "distributed-fight",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how to read this output? Document 3 has a similarity score of \n",
    "# 0.718=72%, document 2 has a similarity score of 42% etc. We\n",
    "# can make this slightly more readable by sorting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "announced-italic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 0.7184812\n",
      "2 0.41707572\n",
      "1 0.32448703\n",
      "0 0.0\n",
      "4 0.0\n",
      "5 0.0\n",
      "6 0.0\n",
      "7 0.0\n",
      "8 0.0\n"
     ]
    }
   ],
   "source": [
    "for document_number, score in sorted(enumerate(sims), key=lambda x: x[1], reverse=True):\n",
    "    print(document_number, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "requested-submission",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary \n",
    "# The core concepts of gensim are:\n",
    "# 1. Document: some text\n",
    "# 2. Corpus: a collection of documents\n",
    "# 3. Vector: a mathematically convenient representation of a document\n",
    "# 4. Model: an algorithm for transforming vectors from \n",
    "# one representation to another. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "improved-variance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We saw these concepts in action. First, we started with a \n",
    "# corpus of documents. Next, we transformed these documents\n",
    "# to a vector space representation. After that, we created \n",
    "# a model that transformed our original vector representation\n",
    "# to Tfldf. Finally, we used our model to calculate the \n",
    "# similarity between some query document and all documents \n",
    "# in the corpus."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
